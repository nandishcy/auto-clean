# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kC5JyyneWaX0IadgndyRSU5emDH1JleY
"""


from timeit import default_timer as timer
import numpy as np
import pandas as pd
from math import isnan
from sklearn import preprocessing
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from loguru import logger
import warnings
warnings.filterwarnings('ignore')
import os
import sys
from timeit import default_timer as timer
import pandas as pd
import numpy as np
import os
import sys
from loguru import logger
import streamlit as st
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score
# ------------------ UI DESIGN (Background + Styling) ------------------

import streamlit as st

def add_bg_design():
    st.markdown(
        """
        <style>
        /* Background image */
        .stApp {
            background-image: url('https://images.unsplash.com/photo-1504384308090-c894fdcc538d');
            background-size: cover;
            background-repeat: no-repeat;
            background-attachment: fixed;
        }

        /* Glass card effect for all content */
        .block-container {
            background: rgba(255, 255, 255, 0.15);
            border-radius: 15px;
            padding: 20px;
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            border: 2px solid rgba(255, 255, 255, 0.4);
        }

        /* Text styling */
        h1, h2, h3, p, label, span {
            color: white !important;
        }

        /* Buttons */
        .stButton>button {
            background-color: rgba(255, 255, 255, 0.8);
            color: black;
            border-radius: 10px;
            padding: 0.6rem 1.2rem;
            border: none;
            font-weight: bold;
            transition: 0.3s;
        }
        .stButton>button:hover {
            background-color: rgba(255, 255, 255, 1);
            transform: scale(1.05);
        }
        </style>
        """,
        unsafe_allow_html=True
    )

# Activate the design
add_bg_design()


class AutoClean:

    def __init__(self, input_data=None, file_path=None, mode='auto', duplicates=False, missing_num=False, missing_categ=False, encode_categ=False, extract_datetime=False, outliers=False, outlier_param=1.5, logfile=True, verbose=False):
        '''
        input_data (dataframe, optional)...Pandas dataframe to be cleaned. If file_path is provided, this argument is ignored.
        file_path (str, optional)........Path to a CSV file to load and clean. If input_data is provided, this argument is ignored.
        mode (str)......................define in which mode you want to run AutoClean
                                        'auto' = sets all parameters to 'auto' and let AutoClean do the data cleaning automatically
                                        'manual' = lets you choose which parameters/cleaning steps you want to perform

        duplicates (str)................define if duplicates in the data should be handled
                                        duplicates are rows where all features are identical
                                        'auto' = automated handling, deletes all copies of duplicates except one
                                        False = skips this step
        missing_num (str)...............define how NUMERICAL missing values are handled
                                        'auto' = automated handling
                                        'linreg' = uses Linear Regression for predicting missing values
                                        'knn' = uses K-NN algorithm for imputation
                                        'mean','median' or 'most_frequent' = uses mean/median/mode imputatiom
                                        'delete' = deletes observations with missing values
                                        False = skips this step
        missing_categ (str).............define how CATEGORICAL missing values are handled
                                        'auto' = automated handling
                                        'logreg' = uses Logistic Regression for predicting missing values
                                        'knn' = uses K-NN algorithm for imputation
                                        'most_frequent' = uses mode imputatiom
                                        'delete' = deletes observations with missing values
                                        False = skips this step
        encode_categ (list).............encode CATEGORICAL features, takes a list as input
                                        ['auto'] = automated encoding
                                        ['onehot'] = one-hot-encode all CATEGORICAL features
                                        ['label'] = label-encode all categ. features
                                        to encode only specific features add the column name or index: ['onehot', ['col1', 2]]
                                        False = skips this step
        extract_datetime (str)..........define whether DATETIME type features should be extracted into separate features
                                        to define granularity set to 'D'= day, 'M'= month, 'Y'= year, 'h'= hour, 'm'= minute or 's'= second
                                        False = skips this step
        outliers (str)..................define how outliers are handled
                                        'winz' = replaces outliers through winsorization
                                        'delete' = deletes observations containing outliers
                                        oberservations are considered outliers if they are outside the lower and upper bound [Q1-1.5*IQR, Q3+1.5*IQR], where IQR is the interquartile range
                                        to set a custom multiplier use the 'outlier_param' parameter
                                        False = skips this step
        outlier_param (int, float)......define the multiplier for the outlier bounds
        logfile (bool)..................define whether to create a logile during the AutoClean process
                                        logfile will be saved in working directory as "autoclean.log"
        verbose (bool)..................define whether AutoClean logs will be printed in console

        OUTPUT (dataframe)..............a cleaned Pandas dataframe, accessible through the 'output' instance
        '''
        start = timer()
        self._initialize_logger(verbose, logfile)

        original_input_data = None # Store the original data for methods like round_values

        if input_data is not None and isinstance(input_data, pd.DataFrame):
            original_input_data = input_data.copy()
            logger.info('Using provided DataFrame for cleaning.')
        elif file_path is not None:
            logger.info(f'Attempting to load data from file path: {file_path}')
            try:
                # Assuming CSV for now, can add logic for other formats later if needed
                # A more robust solution might infer file type or take 'file_type' as param
                original_input_data = pd.read_csv(file_path)
                logger.info(f'Data loaded successfully from {file_path}.')
            except Exception as e:
                logger.error(f'Failed to load data from {file_path}: {e}')
                raise ValueError(f'Could not load data from file_path: {file_path}. Error: {e}')
        else:
            raise ValueError('Either input_data (DataFrame) or file_path must be provided to AutoClean.')

        # The DataFrame that will be cleaned
        output_data = original_input_data.copy()


        if mode == 'auto':
            duplicates, missing_num, missing_categ, outliers, encode_categ, extract_datetime = 'auto', 'auto', 'auto', 'winz', ['auto'], 's'

        self.mode = mode
        self.duplicates = duplicates
        self.missing_num = missing_num
        self.missing_categ = missing_categ
        self.outliers = outliers
        self.encode_categ = encode_categ
        self.extract_datetime = extract_datetime
        self.outlier_param = outlier_param

        # validate the input parameters - pass the data that will be cleaned
        self._validate_params(output_data, verbose, logfile)

        # initialize our class and start the autoclean process
        self.output = self._clean_data(output_data, original_input_data) # Pass output_data for cleaning, original_input_data for reference

        end = timer()
        logger.info('AutoClean process completed in {} seconds', round(end-start, 6))

        if not verbose:
            print('AutoClean process completed in', round(end-start, 6), 'seconds')
        if logfile:
            print('Logfile saved to:', os.path.join(os.getcwd(), 'autoclean.log'))

    def _initialize_logger(self, verbose, logfile):
        logger.remove()
        if verbose:
            logger.add(sys.stderr, level='INFO')
        if logfile:
            logger.add('autoclean.log', level='DEBUG')

    def _validate_params(self, df, verbose, logfile):
        pass # Placeholder for actual parameter validation logic

    def _clean_data(self, df, original_input_data):
        # Instantiate cleaning modules
        missing_values_handler = MissingValues()
        outliers_handler = Outliers()
        adjust_handler = Adjust()
        encode_categ_handler = EncodeCateg()
        duplicates_handler = Duplicates()

        # Set parameters for each handler based on AutoClean's __init__
        missing_values_handler.missing_num = self.missing_num
        missing_values_handler.missing_categ = self.missing_categ
        outliers_handler.outliers = self.outliers
        outliers_handler.outlier_param = self.outlier_param
        adjust_handler.extract_datetime = self.extract_datetime
        adjust_handler.duplicates = self.duplicates # used by round_values logic check
        adjust_handler.missing_num = self.missing_num
        adjust_handler.missing_categ = self.missing_categ
        adjust_handler.outliers = self.outliers
        adjust_handler.encode_categ = self.encode_categ
        adjust_handler.extract_datetime = self.extract_datetime
        encode_categ_handler.encode_categ = self.encode_categ
        duplicates_handler.duplicates = self.duplicates

        # Apply cleaning steps in a defined order
        df = duplicates_handler.handle(df)
        df = missing_values_handler.handle(df)
        df = outliers_handler.handle(df)
        df = adjust_handler.convert_datetime(df)
        df = encode_categ_handler.handle(df)
        df = adjust_handler.round_values(df, original_input_data)

        return df

class MissingValues:

    def handle(self, df, _n_neighbors=3):
        # function for handling missing values in the data
        if self.missing_num or self.missing_categ:
            logger.info('Started handling of missing values...', str(self.missing_num).upper())
            start = timer()
            self.count_missing = df.isna().sum().sum()

            if self.count_missing != 0:
                logger.info('Found a total of {} missing value(s)', self.count_missing)
                df = df.dropna(how='all')
                df.reset_index(drop=True)

                if self.missing_num: # numeric data
                    logger.info('Started handling of NUMERICAL missing values... Method: "{}"', str(self.missing_num).upper())
                    # automated handling
                    if self.missing_num == 'auto':
                        self.missing_num = 'linreg'
                        lr = LinearRegression()
                        df = MissingValues._lin_regression_impute(self, df, lr)
                        self.missing_num = 'knn'
                        imputer = KNNImputer(n_neighbors=_n_neighbors)
                        df = MissingValues._impute(self, df, imputer, type='num')
                    # linear regression imputation
                    elif self.missing_num == 'linreg':
                        lr = LinearRegression()
                        df = MissingValues._lin_regression_impute(self, df, lr)
                    # knn imputation
                    elif self.missing_num == 'knn':
                        imputer = KNNImputer(n_neighbors=_n_neighbors)
                        df = MissingValues._impute(self, df, imputer, type='num')
                    # mean, median or mode imputation
                    elif self.missing_num in ['mean', 'median', 'most_frequent']:
                        imputer = SimpleImputer(strategy=self.missing_num)
                        df = MissingValues._impute(self, df, imputer, type='num')
                    # delete missing values
                    elif self.missing_num == 'delete':
                        df = MissingValues._delete(self, df, type='num')
                        logger.debug('Deletion of {} NUMERIC missing value(s) succeeded', self.count_missing-df.isna().sum().sum())

                if self.missing_categ: # categorical data
                    logger.info('Started handling of CATEGORICAL missing values... Method: "{}"', str(self.missing_categ).upper())
                    # automated handling
                    if self.missing_categ == 'auto':
                        self.missing_categ = 'logreg'
                        lr = LogisticRegression()
                        df = MissingValues._log_regression_impute(self, df, lr)
                        self.missing_categ = 'knn'
                        imputer = KNNImputer(n_neighbors=_n_neighbors)
                        df = MissingValues._impute(self, df, imputer, type='categ')
                    elif self.missing_categ == 'logreg':
                        lr = LogisticRegression()
                        df = MissingValues._log_regression_impute(self, df, lr)
                    # knn imputation
                    elif self.missing_categ == 'knn':
                        imputer = KNNImputer(n_neighbors=_n_neighbors)
                        df = MissingValues._impute(self, df, imputer, type='categ')
                    # mode imputation
                    elif self.missing_categ == 'most_frequent':
                        imputer = SimpleImputer(strategy=self.missing_categ)
                        df = MissingValues._impute(self, df, imputer, type='categ')
                    # delete missing values
                    elif self.missing_categ == 'delete':
                        df = MissingValues._delete(self, df, type='categ')
                        logger.debug('Deletion of {} CATEGORICAL missing value(s) succeeded', self.count_missing-df.isna().sum().sum())
            else:
                logger.debug('{} missing values found', self.count_missing)
            end = timer()
            logger.info('Completed handling of missing values in {} seconds', round(end-start, 6))
        else:
            logger.info('Skipped handling of missing values')
        return df

    def _impute(self, df, imputer, type):
        # function for imputing missing values in the data
        cols_num = df.select_dtypes(include=np.number).columns

        if type == 'num':
            # numerical features
            for feature in df.columns:
                if feature in cols_num:
                    if df[feature].isna().sum().sum() != 0:
                        try:
                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)))
                            counter = df[feature].isna().sum().sum() - df_imputed.isna().sum().sum()

                            # Assign imputed values directly, let round_values handle final type conversion
                            df[feature] = df_imputed

                            if counter != 0:
                                logger.debug('{} imputation of {} value(s) succeeded for feature "{}"', str(self.missing_num).upper(), counter, feature)
                        except:
                            logger.warning('{} imputation failed for feature "{}"', str(self.missing_num).upper(), feature)
        else:
            # categorical features
            for feature in df.columns:
                if feature not in cols_num:
                    if df[feature].isna().sum()!= 0:
                        try:
                            mapping = dict()
                            mappings = {k: i for i, k in enumerate(df[feature].dropna().unique(), 0)}
                            mapping[feature] = mappings
                            df[feature] = df[feature].map(mapping[feature])

                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)), columns=[feature])
                            counter = sum(1 for i, j in zip(list(df_imputed[feature]), list(df[feature])) if i != j)

                            # round to integers before mapping back to original values
                            df[feature] = df_imputed
                            df[feature] = df[feature].round()
                            df[feature] = df[feature].astype('Int64')

                            # map values back to original
                            mappings_inv = {v: k for k, v in mapping[feature].items()}
                            df[feature] = df[feature].map(mappings_inv)
                            if counter != 0:
                                logger.debug('{} imputation of {} value(s) succeeded for feature "{}"', self.missing_categ.upper(), counter, feature)
                        except:
                            logger.warning('{} imputation failed for feature "{}"', str(self.missing_categ).upper(), feature)
        return df

    def _lin_regression_impute(self, df, model):
        # function for predicting missing values with linear regression
        cols_num = df.select_dtypes(include=np.number).columns
        mapping = dict()
        for feature in df.columns:
            if feature not in cols_num:
                # create label mapping for categorical feature values
                mappings = {k: i for i, k in enumerate(df[feature])}
                mapping[feature] = mappings
                df[feature] = df[feature].map(mapping[feature])
        for feature in cols_num:
                try:
                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])
                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])
                    if len(test_df.index) != 0:
                        pipe = make_pipeline(StandardScaler(), model)

                        y = np.log(train_df[feature]) # log-transform the data
                        X_train = train_df.drop(feature, axis=1)
                        test_df.drop(feature, axis=1, inplace=True)

                        try:
                            model = pipe.fit(X_train, y)
                        except:
                            y = train_df[feature] # use non-log-transformed data
                            model = pipe.fit(X_train, y)
                        if (y == train_df[feature]).all():
                            pred = model.predict(test_df)
                        else:
                            pred = np.exp(model.predict(test_df)) # predict values

                        test_df[feature]= pred

                        # Update values directly, let round_values handle final type conversion
                        df[feature].update(test_df[feature])

                        logger.debug('LINREG imputation of {} value(s) succeeded for feature "{}"', len(pred), feature)
                except:
                    logger.warning('LINREG imputation failed for feature "{}"', feature)
        for feature in df.columns:
            try:
                # map categorical feature values back to original
                mappings_inv = {v: k for k, v in mapping[feature].items()}
                df[feature] = df[feature].map(mappings_inv)
            except:
                pass
        return df

    def _log_regression_impute(self, df, model):
        # function for predicting missing values with logistic regression
        cols_num = df.select_dtypes(include=np.number).columns
        mapping = dict()
        for feature in df.columns:
            if feature not in cols_num:
                # create label mapping for categorical feature values
                mappings = {k: i for i, k in enumerate(df[feature])} #.dropna().unique(), 0)}
                mapping[feature] = mappings
                df[feature] = df[feature].map(mapping[feature])

        target_cols = [x for x in df.columns if x not in cols_num]

        for feature in df.columns:
            if feature in target_cols:
                try:
                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])
                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])
                    if len(test_df.index) != 0:
                        pipe = make_pipeline(StandardScaler(), model)

                        y = train_df[feature]
                        train_df.drop(feature, axis=1, inplace=True)
                        test_df.drop(feature, axis=1, inplace=True)

                        model = pipe.fit(train_df, y)

                        pred = model.predict(test_df) # predict values
                        test_df[feature]= pred

                        if (df[feature].fillna(-9999) % 1  == 0).all():
                            # round back to INTs, if original data were INTs
                            test_df[feature] = test_df[feature].round()
                            test_df[feature] = test_df[feature].astype('Int64')
                            df[feature].update(test_df[feature])
                        logger.debug('LOGREG imputation of {} value(s) succeeded for feature "{}"', len(pred), feature)
                except:
                    logger.warning('LOGREG imputation failed for feature "{}"', feature)
        for feature in df.columns:
            try:
                # map categorical feature values back to original
                mappings_inv = {v: k for k, v in mapping[feature].items()}
                df[feature] = df[feature].map(mappings_inv)
            except:
                pass
        return df

    def _delete(self, df, type):
        # function for deleting missing values
        cols_num = df.select_dtypes(include=np.number).columns
        if type == 'num':
            # numerical features
            for feature in df.columns:
                if feature in cols_num:
                    df = df.dropna(subset=[feature])
                    df.reset_index(drop=True)
        else:
            # categorical features
            for feature in df.columns:
                if feature not in cols_num:
                    df = df.dropna(subset=[feature])
                    df.reset_index(drop=True)
        return df

class Outliers:

    def handle(self, df):
        # function for handling of outliers in the data
        if self.outliers:
            logger.info('Started handling of outliers... Method: "{}"', str(self.outliers).upper())
            start = timer()

            if self.outliers in ['auto', 'winz']:
                df = Outliers._winsorization(self, df)
            elif self.outliers == 'delete':
                df = Outliers._delete(self, df)

            end = timer()
            logger.info('Completed handling of outliers in {} seconds', round(end-start, 6))
        else:
            logger.info('Skipped handling of outliers')
        return df

    def _winsorization(self, df):
        # function for outlier winsorization
        cols_num = df.select_dtypes(include=np.number).columns
        for feature in cols_num:
            counter = 0
            # compute outlier bounds
            lower_bound, upper_bound = Outliers._compute_bounds(self, df, feature)
            for row_index, row_val in enumerate(df[feature]):
                if row_val < lower_bound or row_val > upper_bound:
                    if row_val < lower_bound:
                        df.loc[row_index, feature] = lower_bound
                        counter += 1
                    else:
                        df.loc[row_index, feature] = upper_bound
                        counter += 1
            if counter != 0:
                logger.debug('Outlier imputation of {} value(s) succeeded for feature "{}"', counter, feature)
        return df

    def _delete(self, df):
        # function for deleting outliers in the data
        cols_num = df.select_dtypes(include=np.number).columns
        for feature in cols_num:
            counter = 0
            lower_bound, upper_bound = Outliers._compute_bounds(self, df, feature)
            # delete observations containing outliers
            for row_index, row_val in enumerate(df[feature]):
                if row_val < lower_bound or row_val > upper_bound:
                    df = df.drop(row_index)
                    counter +=1
            df = df.reset_index(drop=True)
            if counter != 0:
                logger.debug('Deletion of {} outliers succeeded for feature "{}"', counter, feature)
        return df

    def _compute_bounds(self, df, feature):
        # function that computes the lower and upper bounds for finding outliers in the data
        featureSorted = sorted(df[feature])

        q1, q3 = np.percentile(featureSorted, [25, 75])
        iqr = q3 - q1

        lb = q1 - (self.outlier_param * iqr)
        ub = q3 + (self.outlier_param * iqr)

        return lb, ub

class Adjust:

    def convert_datetime(self, df):
        # function for extracting of datetime values in the data
        if self.extract_datetime:
            logger.info('Started conversion of DATETIME features... Granularity: {}', self.extract_datetime)
            start = timer()
            cols = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns)
            for feature in cols:
                try:
                    # convert features encoded as strings to type datetime ['D','M','Y','h','m','s']
                    df[feature] = pd.to_datetime(df[feature], infer_datetime_format=True)
                    try:
                        df['Day'] = pd.to_datetime(df[feature]).dt.day

                        if self.extract_datetime in ['auto', 'M','Y','h','m','s']:
                            df['Month'] = pd.to_datetime(df[feature]).dt.month

                            if self.extract_datetime in ['auto', 'Y','h','m','s']:
                                df['Year'] = pd.to_datetime(df[feature]).dt.year

                                if self.extract_datetime in ['auto', 'h','m','s']:
                                    df['Hour'] = pd.to_datetime(df[feature]).dt.hour

                                    if self.extract_datetime in ['auto', 'm','s']:
                                        df['Minute'] = pd.to_datetime(df[feature]).dt.minute

                                        if self.extract_datetime in ['auto', 's']:
                                            df['Sec'] = pd.to_datetime(df[feature]).dt.second

                        logger.debug('Conversion to DATETIME succeeded for feature "{}"', feature)

                        try:
                            # check if entries for the extracted dates/times are non-NULL, otherwise drop
                            if (df['Hour'] == 0).all() and (df['Minute'] == 0).all() and (df['Sec'] == 0).all():
                                df.drop('Hour', inplace = True, axis =1 )
                                df.drop('Minute', inplace = True, axis =1 )
                                df.drop('Sec', inplace = True, axis =1 )
                            elif (df['Day'] == 0).all() and (df['Month'] == 0).all() and (df['Year'] == 0).all():
                                df.drop('Day', inplace = True, axis =1 )
                                df.drop('Month', inplace = True, axis =1 )
                                df.drop('Year', inplace = True, axis =1 )
                        except:
                            pass
                    except:
                        # feature cannot be converted to datetime
                        logger.warning('Conversion to DATETIME failed for "{}"', feature)
                except:
                    pass
            end = timer()
            logger.info('Completed conversion of DATETIME features in {} seconds', round(end-start, 4))
        else:
            logger.info('Skipped datetime feature conversion')
        return df

    def round_values(self, df, input_data):
        # function that checks datatypes of features and converts them if necessary
        if self.duplicates or self.missing_num or self.missing_categ or self.outliers or self.encode_categ or self.extract_datetime:
            logger.info('Started feature type conversion...')
            start = timer()
            counter = 0
            cols_num = df.select_dtypes(include=np.number).columns
            for feature in cols_num:
                    # check if all values are integers
                    if (df[feature].fillna(-9999) % 1  == 0).all():
                        try:
                            # encode FLOATs with only 0 as decimals to INT
                            df[feature] = df[feature].astype('Int64')
                            counter += 1
                            logger.debug('Conversion to type INT succeeded for feature "{}"', feature)
                        except:
                            logger.warning('Conversion to type INT failed for feature "{}"', feature)
                    else:
                        try:
                            df[feature] = df[feature].astype(float)
                            # round the number of decimals of FLOATs back to original
                            dec = None
                            for value in input_data[feature]:
                                try:
                                    if dec == None:
                                        dec = str(value)[::-1].find('.')
                                    else:
                                        if str(value)[::-1].find('.') > dec:
                                            dec = str(value)[::-1].find('.')
                                except:
                                    pass
                            df[feature] = df[feature].round(decimals = dec)
                            counter += 1
                            logger.debug('Conversion to type FLOAT succeeded for feature "{}"', feature)
                        except:
                            logger.warning('Conversion to type FLOAT failed for feature "{}"', feature)
            end = timer()
            logger.info('Completed feature type conversion for {} feature(s) in {} seconds', counter, round(end-start, 6))
        else:
            logger.info('Skipped feature type conversion')
        return df

class EncodeCateg:

    def handle(self, df):
        # function for encoding of categorical features in the data
        if self.encode_categ:
            if not isinstance(self.encode_categ, list):
                self.encode_categ = ['auto']
            # select non numeric features
            cols_categ = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns)
            # check if all columns should be encoded
            if len(self.encode_categ) == 1:
                target_cols = cols_categ # encode ALL columns
            else:
                target_cols = self.encode_categ[1] # encode only specific columns
            logger.info('Started encoding categorical features... Method: "{}"', str(self.encode_categ[0]).upper())
            start = timer()
            for feature in target_cols:
                if feature in cols_categ:
                    # columns are column names
                    feature = feature
                else:
                    # columns are indexes
                    feature = df.columns[feature]
                try:
                    # skip encoding of datetime features
                    pd.to_datetime(df[feature])
                    logger.debug('Skipped encoding for DATETIME feature "{}"', feature)
                except:
                    try:
                        if self.encode_categ[0] == 'auto':
                            # ONEHOT encode if not more than 10 unique values to encode
                            if df[feature].nunique() <=10:
                                df = EncodeCateg._to_onehot(self, df, feature)
                                logger.debug('Encoding to ONEHOT succeeded for feature "{}"', feature)
                            # LABEL encode if not more than 20 unique values to encode
                            elif df[feature].nunique() <=20:
                                df = EncodeCateg._to_label(self, df, feature)
                                logger.debug('Encoding to LABEL succeeded for feature "{}"', feature)
                            # skip encoding if more than 20 unique values to encode
                            else:
                                logger.debug('Encoding skipped for feature "{}"', feature)

                        elif self.encode_categ[0] == 'onehot':
                            df = EncodeCateg._to_onehot(self, df, feature)
                            logger.debug('Encoding to {} succeeded for feature "{}"', str(self.encode_categ[0]).upper(), feature)
                        elif self.encode_categ[0] == 'label':
                            df = EncodeCateg._to_label(self, df, feature)
                            logger.debug('Encoding to {} succeeded for feature "{}"', str(self.encode_categ[0]).upper(), feature)
                    except:
                        logger.warning('Encoding to {} failed for feature "{}"', str(self.encode_categ[0]).upper(), feature)
            end = timer()
            logger.info('Completed encoding of categorical features in {} seconds', round(end-start, 6))
        else:
            logger.info('Skipped encoding of categorical features')
        return df

    def _to_onehot(self, df, feature, limit=10):
        # function that encodes categorical features to OneHot encodings
        one_hot = pd.get_dummies(df[feature], prefix=feature)
        if one_hot.shape[1] > limit:
            logger.warning('ONEHOT encoding for feature "{}" creates {} new features. Consider LABEL encoding instead.', feature, one_hot.shape[1])
        # join the encoded df
        df = df.join(one_hot)
        df = df.drop(columns=[feature]) # <--- ADDED: Drop original categorical column
        return df

    def _to_label(self, df, feature):
        # function that encodes categorical features to label encodings
        le = preprocessing.LabelEncoder()

        df[feature + '_lab'] = le.fit_transform(df[feature].values)
        mapping = dict(zip(le.classes_, range(len(le.classes_))))

        for key in mapping:
            try:
                if isnan(key):
                    replace = {mapping[key] : key }
                    df[feature].replace(replace, inplace=True)
            except:
                pass
        df = df.drop(columns=[feature]) # <--- ADDED: Drop original categorical column
        return df

class Duplicates:

    def handle(self, df):
        if self.duplicates:
            logger.info('Started handling of duplicates... Method: "{}"', str(self.duplicates).upper())
            start = timer()
            original = df.shape
            try:
                df.drop_duplicates(inplace=True, ignore_index=False)
                df = df.reset_index(drop=True)
                new = df.shape
                count = original[0] - new[0]
                if count != 0:
                    logger.debug('Deletion of {} duplicate(s) succeeded', count)
                else:
                    logger.debug('{} missing values found', count)
                end = timer()
                logger.info('Completed handling of duplicates in {} seconds', round(end-start, 6))

            except:
                logger.warning('Handling of duplicates failed')
        else:
            logger.info('Skipped handling of duplicates')
        return df

# --- Define RMSE calculation function (if not already in AutoClean scope) ---
def calculate_rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

# --- Streamlit UI for File Upload and Parameter Input ---
st.title("AutoClean - Performance-Driven Data Cleaning App")

st.header("1. Upload Your Dataset")
uploaded_file = st.file_uploader("Choose a CSV or Excel file", type=["csv", "xlsx"])

df = None
if uploaded_file is not None:
    try:
        if uploaded_file.name.endswith('.csv'):
            df = pd.read_csv(uploaded_file)
        elif uploaded_file.name.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(uploaded_file)
        st.success("Dataset loaded successfully!")
        st.write("Original Data Head:", df.head())
    except Exception as e:
        st.error(f"Error loading file: {e}")

if df is not None:
    st.header("2. Configure Cleaning and ML Experiment")

    # Target Column Input
    all_columns = df.columns.tolist()
    target_column_name = st.selectbox(
        "Select your Target Column (y)",
        options=all_columns
    )

    # AutoClean Parameters Input
    st.subheader("AutoClean Strategy Configuration")
    mode_option = st.selectbox("AutoClean Mode", ['auto', 'manual'], index=1) # Default to manual for customization

    autoclean_params = {'mode': mode_option, 'verbose': False}

    if mode_option == 'manual':
        col1, col2 = st.columns(2)
        with col1:
            autoclean_params['missing_num'] = st.selectbox(
                "Handle Numerical Missing Values",
                ['knn', 'mean', 'median', 'most_frequent', 'linreg', 'delete', False],
                index=0 # Default to knn
            )
            autoclean_params['missing_categ'] = st.selectbox(
                "Handle Categorical Missing Values",
                ['most_frequent', 'logreg', 'knn', 'delete', False],
                index=0 # Default to most_frequent
            )
            autoclean_params['duplicates'] = st.selectbox(
                "Handle Duplicates",
                ['auto', False],
                index=0 # Default to auto
            )
        with col2:
            autoclean_params['outliers'] = st.selectbox(
                "Handle Outliers",
                ['winz', 'delete', False],
                index=0 # Default to winz
            )
            autoclean_params['extract_datetime'] = st.selectbox(
                "Extract Datetime Features (Granularity)",
                ['D', 'M', 'Y', 'h', 'm', 's', False],
                index=0 # Default to D
            )
            autoclean_params['encode_categ'] = st.selectbox(
                "Encode Categorical Features",
                [['onehot'], ['label'], ['auto'], False],
                format_func=lambda x: x[0].upper() if isinstance(x, list) else str(x),
                index=0 # Default to onehot
            )
            autoclean_params['outlier_param'] = st.slider("Outlier Multiplier", 0.5, 3.0, 1.5)

    # --- Run AutoClean and Evaluation Logic ---
    if st.button("Run AutoClean Evaluation"):
        st.write("### Running Evaluation...")

        # 1. Prepare Data for ML Experiment
        try:
            y = df[target_column_name]
            X = df.drop(columns=[target_column_name])
        except KeyError:
            st.error(f"Target column '{target_column_name}' not found in the dataset after loading.")
            st.stop()

        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
        st.write(f"Data split: X_train {X_train.shape}, y_train {y_train.shape}, X_val {X_val.shape}, y_val {y_val.shape}")

        # 2. Identify Problem Type and Select Model/Metric
        ml_model = None
        eval_metric_func = None
        metric_name = ''
        if pd.api.types.is_numeric_dtype(y_train) and y_train.nunique() > 2 and (y_train.max() - y_train.min()) > 10:
            problem_type = 'regression'
            ml_model = LinearRegression()
            eval_metric_func = calculate_rmse
            metric_name = 'RMSE'
            st.info("Detected problem type: Regression")
        else:
            problem_type = 'classification'
            if pd.api.types.is_float_dtype(y_train):
                y_train = y_train.astype(int)
                y_val = y_val.astype(int)
            ml_model = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42) # Added random_state for reproducibility
            eval_metric_func = accuracy_score
            metric_name = 'Accuracy'
            st.info(f"Detected problem type: Classification (using {ml_model.__class__.__name__})")

        # Define strategies for evaluation (you might offer multiple via UI or predefine)
        # For simplicity, let's use the autoclean_params directly collected from UI as one strategy
        autoclean_strategies = [
            autoclean_params,
            # You could add other predefined strategies here if you want to compare them always
            {'mode': 'auto', 'verbose': False} # Always compare against AutoClean's auto mode
        ]

        # 3. Implement Cleaning Strategy Evaluation
        results = []
        for i, strategy_params in enumerate(autoclean_strategies):
            st.write(f"--- Evaluating Strategy {i+1} ({strategy_params['mode'].upper()}) ---")

            # Combine ONLY features (X_train and X_val) for consistent AutoClean processing
            combined_X_df = pd.concat([X_train, X_val], axis=0, ignore_index=True)
            len_train = len(X_train) # Store original lengths for splitting back later

            cleaned_combined_X_instance = AutoClean(input_data=combined_X_df.copy(), **strategy_params)
            cleaned_combined_X_output = cleaned_combined_X_instance.output

            # Split the cleaned combined features back into training and validation sets
            X_train_cleaned = cleaned_combined_X_output.iloc[:len_train]
            X_val_cleaned = cleaned_combined_X_output.iloc[len_train:]

            # y_train and y_val remain untouched by AutoClean. Use them directly.
            y_train_cleaned = y_train
            y_val_cleaned = y_val

            # --- ROBUST FEATURE ALIGNMENT AND TYPE HANDLING ---
            X_train_cleaned = X_train_cleaned.drop(columns=['datetime_col'], errors='ignore')
            X_val_cleaned = X_val_cleaned.drop(columns=['datetime_col'], errors='ignore')

            # Now all_final_cols should be consistent because AutoClean was run on combined data
            for col in X_train_cleaned.columns:
                if not pd.api.types.is_numeric_dtype(X_train_cleaned[col]):
                    X_train_cleaned[col] = pd.to_numeric(X_train_cleaned[col], errors='coerce').fillna(0)
            for col in X_val_cleaned.columns:
                if not pd.api.types.is_numeric_dtype(X_val_cleaned[col]):
                    X_val_cleaned[col] = pd.to_numeric(X_val_cleaned[col], errors='coerce').fillna(0)

            X_train_cleaned = X_train_cleaned.select_dtypes(include=np.number)
            X_val_cleaned = X_val_cleaned.select_dtypes(include=np.number)

            # Ensure columns are aligned after all numeric conversions
            all_final_cols = X_train_cleaned.columns.union(X_val_cleaned.columns)
            X_train_cleaned = X_train_cleaned.reindex(columns=all_final_cols, fill_value=0)
            X_val_cleaned = X_val_cleaned.reindex(columns=all_final_cols, fill_value=0)

            # Debugging print statements to verify columns before fit/predict
            # st.write(f"DEBUG - Strategy {i+1} X_train_cleaned columns ({len(X_train_cleaned.columns)}): {sorted(X_train_cleaned.columns.tolist())}")
            # st.write(f"DEBUG - Strategy {i+1} X_val_cleaned columns ({len(X_val_cleaned.columns)}): {sorted(X_val_cleaned.columns.tolist())}")

            if len(X_train_cleaned.columns) == 0 or len(X_val_cleaned.columns) == 0:
                st.warning(f"Strategy {i+1}: No numeric features left after cleaning. Skipping model training.")
                metric_value = float('nan')
            else:
                # Train the dynamically selected model
                ml_model.fit(X_train_cleaned, y_train_cleaned)

                # Make predictions
                y_pred = ml_model.predict(X_val_cleaned)

                # Calculate the dynamically selected metric
                metric_value = eval_metric_func(y_val_cleaned, y_pred)

            # Store results
            results.append({
                'strategy_params': strategy_params,
                metric_name: metric_value
            })

        # 4. Select Optimal Cleaning Parameters
        st.header("3. Evaluation Results")
        if results:
            best_metric_value = -float('inf') if metric_name == 'Accuracy' else float('inf')
            best_strategy = None

            for r in results:
                if metric_name == 'Accuracy':
                    if r[metric_name] > best_metric_value:
                        best_metric_value = r[metric_name]
                        best_strategy = r['strategy_params']
                else: # RMSE
                    if r[metric_name] < best_metric_value:
                        best_metric_value = r[metric_name]
                        best_strategy = r['strategy_params']

            st.subheader(f"Evaluation Results ({metric_name}):")
            for r in results:
                st.write(f"Strategy: {r['strategy_params']} - {metric_name}: {r[metric_name]:.4f}")

            st.subheader("Optimal AutoClean Strategy:")
            st.success(f"Parameters: {best_strategy}")
            st.success(f"Achieved {metric_name}: {best_metric_value:.4f}")
        else:
            st.warning("No results to display.")

        st.subheader("4. Cleaned Data Output")
        # Display the head of the cleaned data (using the cleaned_combined_X_output from the BEST strategy or the last one)
        # For simplicity, let's re-run the AutoClean with the best strategy to get the final cleaned data for display/download
        if best_strategy:
            st.write(f"Applying the optimal strategy: {best_strategy}")
            final_cleaned_instance = AutoClean(input_data=df.copy().drop(columns=[target_column_name]), **best_strategy)
            final_cleaned_df_X = final_cleaned_instance.output

            st.write("Cleaned Features Head:", final_cleaned_df_X.head())

            # For download, you might want to re-combine with the target if it's not encoded
            final_cleaned_df_with_target = pd.concat([final_cleaned_df_X, df[target_column_name]], axis=1)

            # Provide download button
            csv_data = final_cleaned_df_with_target.to_csv(index=False)
            st.download_button(
                label="Download Cleaned Data as CSV",
                data=csv_data,
                file_name="cleaned_data.csv",
                mime="text/csv",
            )
        else:
            st.warning("Could not determine best strategy to display cleaned data.")
